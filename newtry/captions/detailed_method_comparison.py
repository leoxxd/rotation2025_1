#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†å¯¹æ¯”åŸå§‹NSDé¡¹ç›®å’Œæˆ‘ä»¬çš„embeddingç”Ÿæˆæ–¹æ³•
é‡ç‚¹åˆ†æï¼šå•è¯çº§åˆ«çš„embeddingç”Ÿæˆæ–¹å¼
"""

import os
import pickle
import numpy as np
import nltk

def analyze_embedding_generation_methods():
    """åˆ†æä¸¤ç§embeddingç”Ÿæˆæ–¹æ³•çš„å…³é”®å·®å¼‚"""
    
    print("=" * 80)
    print("Embeddingç”Ÿæˆæ–¹æ³•è¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    print("\n1. åŸå§‹NSDé¡¹ç›®çš„æ–¹æ³•:")
    print("   - ä½¿ç”¨ get_word_embedding(word, embeddings, EMBEDDING_TYPE)")
    print("   - å¯¹äºsentence transformer: get_embeddings([word], embeddings, embedding_type)[0]")
    print("   - å…³é”®: å°†å•ä¸ªå•è¯ä½œä¸ºåˆ—è¡¨ [word] è¾“å…¥ç»™æ¨¡å‹")
    print("   - æ¨¡å‹å¤„ç†: embedding_model.encode([word])")
    print("   - ç»“æœ: è¿”å›è¯¥å•è¯çš„embeddingå‘é‡")
    
    print("\n2. æˆ‘ä»¬çš„æ–¹æ³•:")
    print("   - ä½¿ç”¨ self.model.encode([word], convert_to_tensor=False)[0]")
    print("   - å…³é”®: åŒæ ·å°†å•ä¸ªå•è¯ä½œä¸ºåˆ—è¡¨ [word] è¾“å…¥ç»™æ¨¡å‹")
    print("   - æ¨¡å‹å¤„ç†: SentenceTransformer.encode([word])")
    print("   - ç»“æœ: è¿”å›è¯¥å•è¯çš„embeddingå‘é‡")
    
    print("\n3. å…³é”®å‘ç°:")
    print("   âœ… ä¸¤ç§æ–¹æ³•åœ¨å•è¯çº§åˆ«embeddingç”Ÿæˆä¸Šæ˜¯ç›¸åŒçš„ï¼")
    print("   âœ… éƒ½æ˜¯å°†å•ä¸ªå•è¯ä½œä¸ºåˆ—è¡¨è¾“å…¥ç»™sentence transformer")
    print("   âœ… éƒ½æ˜¯å¯¹å•è¯embeddingså–å¹³å‡")
    
    print("\n4. é‚£ä¹ˆé—®é¢˜åœ¨å“ªé‡Œï¼Ÿ")
    print("   ğŸ” é—®é¢˜å¯èƒ½åœ¨äº:")
    print("   - åˆ†è¯æ–¹å¼çš„å·®å¼‚")
    print("   - è¯æ±‡ä¿®æ­£çš„ç¼ºå¤±")
    print("   - æ•°æ®é¢„å¤„ç†çš„ä¸åŒ")

def test_word_embedding_consistency():
    """æµ‹è¯•å•è¯embeddingçš„ä¸€è‡´æ€§"""
    print("\n" + "=" * 80)
    print("å•è¯Embeddingä¸€è‡´æ€§æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•å•è¯
    test_words = ['food', 'plate', 'vegetables', 'garnish', 'stir', 'fry']
    
    print("\nç†è®ºä¸Šï¼Œä¸¤ç§æ–¹æ³•å¯¹ç›¸åŒå•è¯åº”è¯¥äº§ç”Ÿç›¸åŒçš„embedding:")
    print("åŸå§‹æ–¹æ³•: get_embeddings([word], model, 'all-mpnet-base-v2')[0]")
    print("æˆ‘ä»¬çš„æ–¹æ³•: model.encode([word])[0]")
    print("è¿™ä¸¤ç§æ–¹å¼åº”è¯¥æ˜¯ç­‰ä»·çš„ï¼")

def analyze_tokenization_impact():
    """åˆ†æåˆ†è¯å¯¹embeddingçš„å½±å“"""
    print("\n" + "=" * 80)
    print("åˆ†è¯å¯¹Embeddingçš„å½±å“åˆ†æ")
    print("=" * 80)
    
    test_sentence = "Food on a white plate with vegetables and a garnish."
    
    print(f"\næµ‹è¯•å¥å­: '{test_sentence}'")
    
    # åŸå§‹æ–¹æ³•åˆ†è¯
    original_tokens = nltk.word_tokenize(test_sentence)
    print(f"\nåŸå§‹æ–¹æ³•åˆ†è¯ç»“æœ:")
    print(f"  {original_tokens}")
    print(f"  å•è¯æ•°é‡: {len(original_tokens)}")
    
    # æˆ‘ä»¬çš„æ–¹æ³•åˆ†è¯
    our_tokens = nltk.word_tokenize(test_sentence.lower())
    our_filtered_tokens = [token for token in our_tokens if len(token) > 1 and token.isalpha()]
    print(f"\næˆ‘ä»¬çš„æ–¹æ³•åˆ†è¯ç»“æœ:")
    print(f"  {our_filtered_tokens}")
    print(f"  å•è¯æ•°é‡: {len(our_filtered_tokens)}")
    
    # åˆ†æå·®å¼‚
    print(f"\nå·®å¼‚åˆ†æ:")
    print(f"  ä¸¢å¤±çš„å•è¯: {set(original_tokens) - set(our_filtered_tokens)}")
    print(f"  æ–°å¢çš„å•è¯: {set(our_filtered_tokens) - set(original_tokens)}")
    print(f"  å…±åŒå•è¯: {set(original_tokens) & set(our_filtered_tokens)}")
    
    # è®¡ç®—å·®å¼‚æ¯”ä¾‹
    common_words = len(set(original_tokens) & set(our_filtered_tokens))
    total_original = len(original_tokens)
    similarity = common_words / total_original
    print(f"  å•è¯é‡å ç‡: {similarity:.2%}")

def analyze_embedding_statistics():
    """åˆ†æembeddingç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("Embeddingç»Ÿè®¡ä¿¡æ¯åˆ†æ")
    print("=" * 80)
    
    try:
        # åŠ è½½æˆ‘ä»¬çš„embedding
        word_avg_emb = np.load("embeddings_output/word_average_embeddings.npy")
        image_emb = np.load("embeddings_output/image_embeddings.npy")
        
        print(f"\næˆ‘ä»¬çš„å•è¯å¹³å‡embedding:")
        print(f"  å½¢çŠ¶: {word_avg_emb.shape}")
        print(f"  å‡å€¼: {word_avg_emb.mean():.6f}")
        print(f"  æ ‡å‡†å·®: {word_avg_emb.std():.6f}")
        print(f"  æœ€å°å€¼: {word_avg_emb.min():.6f}")
        print(f"  æœ€å¤§å€¼: {word_avg_emb.max():.6f}")
        
        print(f"\nå®Œæ•´caption embedding:")
        print(f"  å½¢çŠ¶: {image_emb.shape}")
        print(f"  å‡å€¼: {image_emb.mean():.6f}")
        print(f"  æ ‡å‡†å·®: {image_emb.std():.6f}")
        print(f"  æœ€å°å€¼: {image_emb.min():.6f}")
        print(f"  æœ€å¤§å€¼: {image_emb.max():.6f}")
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(word_avg_emb.flatten(), image_emb.flatten())[0, 1]
        print(f"\nç›¸å…³æ€§åˆ†æ:")
        print(f"  å•è¯å¹³å‡embeddingä¸å®Œæ•´caption embeddingçš„ç›¸å…³æ€§: {correlation:.6f}")
        
        # åˆ†æç›¸å…³æ€§ä½çš„åŸå› 
        print(f"\nç›¸å…³æ€§ä½çš„åŸå› åˆ†æ:")
        if correlation < 0.3:
            print("  âš ï¸ ç›¸å…³æ€§è¿‡ä½ï¼Œå¯èƒ½åŸå› :")
            print("  1. åˆ†è¯æ–¹å¼å·®å¼‚å¯¼è‡´å•è¯é›†åˆä¸åŒ")
            print("  2. å¤§å°å†™è½¬æ¢å½±å“embeddingè´¨é‡")
            print("  3. æ ‡ç‚¹ç¬¦å·è¿‡æ»¤è¿‡äºæ¿€è¿›")
            print("  4. ç¼ºä¹è¯æ±‡ä¿®æ­£æœºåˆ¶")
        
    except FileNotFoundError as e:
        print(f"  æ–‡ä»¶æœªæ‰¾åˆ°: {e}")

def analyze_word_lists_difference():
    """åˆ†æå•è¯åˆ—è¡¨çš„å·®å¼‚"""
    print("\n" + "=" * 80)
    print("å•è¯åˆ—è¡¨å·®å¼‚åˆ†æ")
    print("=" * 80)
    
    try:
        with open("embeddings_output/word_lists.pkl", 'rb') as f:
            word_lists = pickle.load(f)
        
        print(f"\nå•è¯åˆ—è¡¨ç»Ÿè®¡:")
        print(f"  æ€»å›¾åƒæ•°: {len(word_lists)}")
        
        # ç»Ÿè®¡æ¯å¼ å›¾åƒçš„å•è¯æ•°
        word_counts = [len(words) for words in word_lists]
        print(f"  å¹³å‡æ¯å›¾åƒå•è¯æ•°: {np.mean(word_counts):.2f}")
        print(f"  å•è¯æ•°èŒƒå›´: {min(word_counts)} - {max(word_counts)}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå›¾åƒçš„å•è¯
        print(f"\nå‰3ä¸ªå›¾åƒçš„å•è¯:")
        for i in range(3):
            print(f"  å›¾åƒ {i}: {word_lists[i][:20]}...")
        
        # åˆ†æå•è¯ç±»å‹
        all_words = []
        for words in word_lists:
            all_words.extend(words)
        
        unique_words = set(all_words)
        print(f"\nè¯æ±‡ç»Ÿè®¡:")
        print(f"  æ€»å•è¯æ•°: {len(all_words)}")
        print(f"  å”¯ä¸€å•è¯æ•°: {len(unique_words)}")
        print(f"  å¹³å‡æ¯å•è¯å‡ºç°æ¬¡æ•°: {len(all_words)/len(unique_words):.2f}")
        
    except FileNotFoundError:
        print("  æœªæ‰¾åˆ°word_lists.pklæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    analyze_embedding_generation_methods()
    test_word_embedding_consistency()
    analyze_tokenization_impact()
    analyze_embedding_statistics()
    analyze_word_lists_difference()
    
    print("\n" + "=" * 80)
    print("ç»“è®ºå’Œå»ºè®®")
    print("=" * 80)
    print("""
å…³é”®å‘ç°:
1. ä¸¤ç§æ–¹æ³•åœ¨å•è¯çº§åˆ«embeddingç”Ÿæˆä¸Šæ˜¯ç›¸åŒçš„
2. éƒ½æ˜¯å°†å•ä¸ªå•è¯ä½œä¸ºåˆ—è¡¨è¾“å…¥ç»™sentence transformer
3. éƒ½æ˜¯å¯¹å•è¯embeddingså–å¹³å‡

é—®é¢˜æ ¹æº:
1. åˆ†è¯æ–¹å¼å·®å¼‚: æˆ‘ä»¬ä¸¢å¤±äº†æ ‡ç‚¹ç¬¦å·å’ŒçŸ­è¯
2. å¤§å°å†™è½¬æ¢: å¯èƒ½å½±å“embeddingè´¨é‡
3. ç¼ºä¹è¯æ±‡ä¿®æ­£: åŸå§‹æ–¹æ³•æœ‰verb_adjustmentsæœºåˆ¶

è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨ä¸åŸå§‹æ–¹æ³•ä¸€è‡´çš„åˆ†è¯æ–¹å¼
2. ä¸è¿›è¡Œå¤§å°å†™è½¬æ¢
3. ä¸è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
4. æ·»åŠ è¯æ±‡ä¿®æ­£æœºåˆ¶

è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆç›¸å…³æ€§è¿™ä¹ˆä½ - ä¸æ˜¯embeddingç”Ÿæˆæ–¹å¼çš„é—®é¢˜ï¼Œ
è€Œæ˜¯æ•°æ®é¢„å¤„ç†ï¼ˆåˆ†è¯ï¼‰çš„é—®é¢˜ï¼
""")

if __name__ == "__main__":
    main()
